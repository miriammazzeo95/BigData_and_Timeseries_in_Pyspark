{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark_ConfigurationImportsFunctions",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/Pyspark_ConfigurationImportsFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwb1i76uVvDY"
      },
      "source": [
        "# **Set Up Pyspark inÂ Colab**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgGt5enwCeoi",
        "outputId": "c428af98-6475-4c41-f4ea-f3c182103ceb"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "#https://stackoverflow.com/questions/55240940/error-while-installing-spark-on-google-colab\n",
        "%cd /\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] =  \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg0Xkp3FCftD"
      },
      "source": [
        "from functools import *\n",
        "import itertools \n",
        "from itertools import chain\n",
        "import os\n",
        "import subprocess\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import sys, math\n",
        "\n",
        "import datetime\n",
        "from datetime import *\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "# **Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srX13nl-6YId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09807121-2adf-475b-97fb-c096d6fe95b2"
      },
      "source": [
        "print('start file')\n",
        "from functools import *\n",
        "import itertools \n",
        "from itertools import chain\n",
        "import os\n",
        "import subprocess\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import sys, math\n",
        "\n",
        "import datetime\n",
        "from datetime import *\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDgScVs95_z6"
      },
      "source": [
        "def read_timeseries(dump_freq: str, year_month: tuple,  day=None):\n",
        "    if day:\n",
        "        data = spark.read.parquet(f\"/feddl/landing/trefor/quarter_hourly/{dump_freq}/year={year_month[0]}/month={year_month[1]}/day={day}\")\n",
        "    else:\n",
        "        data = spark.read.parquet(f\"/feddl/landing/trefor/quarter_hourly/{dump_freq}/year={year_month[0]}/month={year_month[1]}\")\n",
        "    return data\n",
        "\n",
        "####################################################################################################################################    \n",
        "\n",
        "def df_filterByColumnValue(df, col_name: str, col_item: str):\n",
        "    # filters dataframe rows on a specific string in a column\n",
        "    df = df.filter(col(col_name) == col_item)\n",
        "    \n",
        "    return df\n",
        "\n",
        "####################################################################################################################################\n",
        "\n",
        "def forward_fill_int(df, col_name, order_col_name, fill_col_name):\n",
        "    \n",
        "    '''Forward fill a column by a column/set of columns (order_col).  \n",
        "    Parameters:\n",
        "    ------------\n",
        "    df: Dataframe \n",
        "    col_name: String, column of  to be cleaned\n",
        "    order_col: String, column names (part of df) selected to order the window\n",
        "    fill_col_name: String, name of the new cleaned column\n",
        "    Return:\n",
        "    ---------\n",
        "    df: Dataframe \n",
        "        Return df with the filled_cols. \n",
        "    +---------+-------------------+-------+\n",
        "    |cookie_ID|               Time|User_ID|\n",
        "    +---------+-------------------+-------+\n",
        "    |        1|2015/12/01 00:15:00|   55.8|\n",
        "    |        1|2015/12/01 00:30:00|   null|\n",
        "    |        1|2015/12/01 01:00:00|   -7.0|\n",
        "    |        1|2015/12/01 02:00:00|  29.23|\n",
        "    |        1|2015/12/01 01:45:00|   32.0|\n",
        "    |        1|2015/12/01 05:00:00|   24.5|\n",
        "    |        2|2015/12/01 03:30:00|   39.0|\n",
        "    |        2|2015/12/01 02:45:00|   52.0|\n",
        "    |        2|2015/12/01 03:15:00|   53.0|\n",
        "    |        2|2015/12/01 02:15:00|  250.0|\n",
        "    +---------+-------------------+-------+\n",
        "    '''\n",
        "    \n",
        "    #change column type to Double\n",
        "    df = df.withColumn(col_name, df[col_name].cast(DoubleType()))\n",
        "   \n",
        "    col_var = df.agg({col_name : 'variance'}).first()[0]\n",
        "    col_stdev =  col_var**(1/2)\n",
        "    col_mean = df.agg(avg(col(col_name))).first()[0]\n",
        "\n",
        "    # \"value\" and \"tmp\" are temporary columns created ton enable forward fill. \n",
        "    # mapping null values and outliers with zeroes\n",
        "    df = df.withColumn('value', when((col(col_name) >= (col_mean + 2*col_stdev)) | (col(col_name) <= (col_mean - 2*col_stdev)) | col(col_name).isNull(), 0).otherwise(1)) \n",
        "    \n",
        "    # Find cumulative sum of a the 'value' column\n",
        "    df = df.withColumn('tmp', lit('tmp')) \n",
        "    windowval = (Window.partitionBy('tmp') \n",
        "                 .orderBy(df[order_col_name[0]].asc(), df[order_col_name[1]].asc())\n",
        "                 .rangeBetween(Window.unboundedPreceding, 0)) \n",
        "    df = df.withColumn('cum_sum', sum('value').over(windowval).alias('cumsum'))\n",
        "    df = df.drop('tmp') \n",
        "    df = df.drop('value')\n",
        "\n",
        "    # forward filling \n",
        "    win = (Window.partitionBy('cum_sum').orderBy(order_col_name))\n",
        "    df = df.withColumn(fill_col_name, collect_list(col_name).over(win)[0])\n",
        "\n",
        "    df = df.drop('cum_sum') \n",
        "\n",
        "    \n",
        "    return df   \n",
        " \n",
        "\n",
        "####################################################################################################################################    \n",
        "\n",
        "def forward_fill_str(df, col_name, order_col_name, fill_col_name):\n",
        "    \n",
        "    '''Forward fill a column by a column/set of columns (order_col).  \n",
        "    Parameters:\n",
        "    ------------\n",
        "    df: Dataframe \n",
        "    col_name: String, column of strings that we want to 'clean' with forward fille\n",
        "    order_col: tuple of String, column names (part of df) selected to order the window\n",
        "    fill_col_name: String, name of the new cleaned column\n",
        "    Return:\n",
        "    ---------\n",
        "    df: Dataframe \n",
        "        Return df with the filled_cols. \n",
        "    '''\n",
        "    \n",
        "    #change column type to String\n",
        "    df = df.withColumn(col_name, df[col_name].cast(\"string\"))\n",
        "    \n",
        "    # \"value\" and \"tmp\" are temporary columns created ton enable forward fill. \n",
        "    # mapping null values and outliers with zeroes\n",
        "    df = df.withColumn('value', when((col(col_name) == \"?\") | (col(col_name) == \" \") | (col(col_name) == \"\") | (col(col_name).contains(\"?\")) | (col(col_name) == '?') | (col(col_name).isNull()) | (col(col_name).rlike('[\\?]')), 0).otherwise(1)) \n",
        "    \n",
        "    # Find cumulative sum of a the 'value' column\n",
        "    df = df.withColumn('tmp', lit('tmp')) \n",
        "    windowval = (Window.partitionBy('tmp') \n",
        "                 .orderBy(df[order_col_name[0]].asc(), df[order_col_name[1]].asc())\n",
        "                 .rangeBetween(Window.unboundedPreceding, 0)) \n",
        "    df = df.withColumn('cum_sum', sum('value').over(windowval).alias('cumsum'))\n",
        "    df = df.drop('tmp') \n",
        "    df = df.drop('value')\n",
        "\n",
        "    # forward filling \n",
        "    win = (Window.partitionBy('cum_sum').orderBy(order_col_name[0]))\n",
        "    df = df.withColumn(fill_col_name, collect_list(col_name).over(win)[0])\n",
        "\n",
        "    df = df.drop('cum_sum') \n",
        "    \n",
        "    return df   \n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def col_to_hour(df, column_name: str, new_column_name: str):\n",
        "    \"\"\"\n",
        "    000 -> 0.00 -> 00:00:00, 630 -> 6.30 -> 06:00:00, 45 -> 0.45 -> 00:00:00, 12.15 -> 12:00:00\n",
        "    \"\"\"\n",
        "    df = df.withColumn(column_name, col(column_name)/100)\n",
        "    df = df.withColumn(column_name, col(column_name).cast(StringType()))\n",
        "    df = df.withColumn(column_name, substring_index(col(column_name), \".\", 1))\n",
        "    df = df.withColumn(new_column_name, date_format(to_timestamp(col(column_name),\"H\"), \"HH:mm:ss\"))\n",
        "    \n",
        "    return df\n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def df_mapCol(df, df_dict, new_col, key_col):\n",
        "    def translate(mapping):\n",
        "        def translate_(col):\n",
        "            return mapping.get(col)\n",
        "        return udf(translate_, StringType())\n",
        "    \n",
        "    df_remapped = df.withColumn(new_col, translate(df_dict)(key_col))\n",
        "    return df_remapped\n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def createStationCols(df):\n",
        "    df = df.withColumn(\"MissingColumns\", split(\"MissingColumns\", \",\"))\n",
        "    df = df.select(*df.columns[:-1], df.MissingColumns[0].alias('Station1'), df.MissingColumns[1].alias('Station2'), df.MissingColumns[2].alias('Station3'), df.MissingColumns[3].alias('Station4'))\n",
        "    df = df.withColumn(\"Station1\", regexp_replace(\"Station1\", \"\\W+\", \"\"))\n",
        "    df = df.withColumn(\"Station4\", regexp_replace(\"Station4\", \"\\W+\", \"\"))\n",
        "    return df\n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def merge_cols(df, column_name: str, columnToDrop_name: str, new_column_name: str, sep=' '):\n",
        "    \"\"\"\n",
        "    merging two string columns in a new column with name column_name3, separeted by sep (a space by default)\n",
        "    the new column is inserted in the df first place \n",
        "    \"\"\"\n",
        "    df=df.withColumn(new_column_name, concat_ws(sep, df[column_name], df[columnToDrop_name]))\n",
        "       \n",
        "    df=df.drop(columnToDrop_name)\n",
        "    return df\n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def dfCol_to_time(df, column_name: str, format = '%H.%M'):\n",
        "    \"\"\"\n",
        "    takes a df, a column name and the time format of the column values \n",
        "    if time values are such as\n",
        "    0, 630, 45... \n",
        "    the values are mapped as\n",
        "    0 -> 0.00, 630 -> 6.30, 45 -> 0.45, 1215 -> 12.15\n",
        "    and transformed in data-time format '%H.%M' such as \n",
        "    0.00 -> 00:00:00, 6.30 -> 06:30:00, 0.45 -> 00:45:00, 12.15 -> 12:15:00\n",
        "    \"\"\"\n",
        "    # dividing each value in column_name by 100\n",
        "    df = df.withColumn(column_name, df[column_name]/100)\n",
        "    \n",
        "    # increasing decimal precision to 2 decimals for each value in column_name\n",
        "    df = df.withColumn(column_name, df[column_name].cast(DecimalType(10,2)))\n",
        "    \n",
        "    # converting int to str in column_name\n",
        "    df = df.withColumn(column_name, df[column_name].cast(StringType()))\n",
        "    \n",
        "    # mapping with lambda expreassion each value to format in column_name to data-time type \n",
        "    #Source func: https://stackoverflow.com/questions/45031810/replace-substring-of-values-in-a-dataframe-in-pyspark\n",
        "    func = UserDefinedFunction(lambda x: datetime.strptime(x, format), TimestampType())\n",
        "    df = df.withColumn(column_name, func(col(column_name)))\n",
        "    \n",
        "    # changing data-time format to only time\n",
        "    # Source: https://stackoverflow.com/questions/63691162/how-to-extract-time-from-timestamp-in-pyspark\n",
        "    df = df.withColumn(column_name, date_format(column_name, 'HH:mm:ss'))\n",
        "    \n",
        "    return df\n",
        "    \n",
        "####################################################################################################################################   \n",
        "\n",
        "def get_filePaths(shell_cmd):\n",
        "    \"\"\"\n",
        "    takes in imput a string containing a shell path-reading command, for example: 'hdfs dfs -ls /feddl/landing/trefor/...'\n",
        "    returns a list of str whith file paths contained in the path specified in input\n",
        "    \"\"\"\n",
        "    \n",
        "    paths = subprocess.run(shell_cmd, shell=True, universal_newlines = True, stdout = subprocess.PIPE).stdout.splitlines()\n",
        "    paths = [p.split()[-1] for p in paths]\n",
        "    paths = [p for p in paths if p[0:6] == '/feddl']\n",
        "    paths = [p for p in paths if p[-1:] != 'S']\n",
        "    \n",
        "    return paths\n",
        "    \n",
        "# TEST - get all file path in Trefor landing zone\n",
        "# all_trefor_days = get_filePaths('hdfs dfs -ls /feddl/landing/trefor/quarter_hourly/*/*/*/*/*/')\n",
        "\n",
        "####################################################################################################################################  \n",
        "\n",
        "def df_to_dict(path):\n",
        "    # READ DF DICTIONARY (SAVED PREVIOUSLY) IN 'feddl/work/CDK/miriam/dictionary/'\n",
        "    df_dict = spark.read.parquet(path)\n",
        "    #df_dict.explain(True)\n",
        "    df_dict = df_dict.withColumn(\"InstallationKabelskab\", col('InstallationKabelskab').cast('int'))\n",
        "\n",
        "    # SORTING ALPHABETICALLY\n",
        "    df_dict = df_dict.sort(col(\"InstallationAdresse\").asc())\n",
        "\n",
        "    # CONVERT TO REAL DICTIONARY \n",
        "    dictionary = {row['InstallationAdresse']:row['InstallationKabelskab'] for row in df_dict.collect()}\n",
        "    return dictionary\n",
        "\n",
        "####################################################################################################################################  \n",
        "\n",
        "def rename_columns(df, dict_col):\n",
        "    \"\"\"\n",
        "    takes a df and a dictionary as {oldName:newName}\n",
        "    \"\"\"\n",
        "    for col in df.schema.names:\n",
        "        df = df.withColumnRenamed(col,dict_col[col])\n",
        "    return df\n",
        "    \n",
        "def add_index_column(df, position = \"last\"):\n",
        "    \"\"\"\n",
        "    add monotonous incresingly index (UID) as first or last column\n",
        "    \"\"\"\n",
        "    original_columns = df.columns\n",
        "    df_with_index = df.select(\"*\").withColumn(\"UID\", monotonically_increasing_id())\n",
        "    \n",
        "    if position == \"last\":\n",
        "        return df_with_index\n",
        "        \n",
        "    elif position == \"first\":\n",
        "        new_list_columns = [\"UID\"]\n",
        "        new_list_columns.extend(original_columns)\n",
        "        df_with_index = df_with_index.select(new_list_columns)\n",
        "        return df_with_index\n",
        "    else: \n",
        "        print (\"not implemented, select first or last\")\n",
        "        \n",
        "def remove_cols(df,drop_cols, multiple = True):\n",
        "    \"\"\"\n",
        "    remove one or multiple columns from dataframe \n",
        "    \"\"\"\n",
        "    if multiple:\n",
        "        df = df.drop(*drop_cols)\n",
        "    else:\n",
        "        #df = df.drop(col(f\"{drop_cols}\"))\n",
        "        df = df.drop(col(drop_cols))\n",
        "    return df\n",
        "\n",
        "\n",
        "####################################################################################################################################   \n",
        "\n",
        "def remove_file(file_path):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    \n",
        "    subprocess.run(f'hdfs dfs -rm -r {file_path}', shell=True, universal_newlines = True, stdout = subprocess.PIPE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxg6CW4OzrrH"
      },
      "source": [
        ""
      ]
    }
  ]
}