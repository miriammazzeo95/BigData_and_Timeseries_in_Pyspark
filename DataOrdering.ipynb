{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataOrdering",
      "provenance": [],
      "collapsed_sections": [
        "sq8U3BtmhtRx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/DataOrdering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "a0543c15-c6c4-40e2-a284-0adb52e8165b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIV8oOhQsnUA"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_ConfigurationImportsFunctions.ipynb') # creates a local copy in the VM\n",
        "!jupyter nbconvert --to python 'Pyspark_ConfigurationImportsFunctions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_ConfigurationImportsFunctions.ipynb # deletes the local copy\n",
        "import Pyspark_ConfigurationImportsFunctions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PoAow1Gro0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXiHiN4vKHXf"
      },
      "source": [
        "# **Ordering Time Series file in folders**\n",
        "\n",
        "> This script is meant to 'reorder' files to the right monthly and daily folder. The code process parquet data files, collected daily by meters since 4 years. The files are contained in month and day folders, in theese folders files with wrong timestamp are contained as well. The code process files relatively slow to avoid exceeding memory \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKdqYpqNKUmQ"
      },
      "source": [
        "# THE DATASET HAS COLUMNS THAT CHANGE OVER TIME\n",
        "# this function checks the number of columns and fix them adding missing columns with empty values\n",
        "'''\n",
        "['Dato', 'Time-Kvarter', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'ProduktkomponentType', 'Dataset', 'day'] ->\n",
        "['Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset', 'day']\n",
        "'''\n",
        "def fix_cols(df, path):\n",
        "    value = ''\n",
        "    if len(df.columns)==11:\n",
        "        \n",
        "        df = df.withColumn('Produktkomponent',lit(value))\\\n",
        "            .withColumn('MeterKey', lit(value))\\\n",
        "            .withColumn('InstallationKabelskab',lit(value))\\\n",
        "            .withColumn('InstallationLS_Udføring',lit(value))\\\n",
        "            .withColumn('InstallationNetstation',lit(value))\\\n",
        "            .withColumn('InstallationStation60KV',lit(value))\\\n",
        "            .withColumn('Type',lit(value))\\\n",
        "            .select( 'Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset')\n",
        "        return df\n",
        "    \n",
        "    elif len(df.columns)==14:\n",
        "\n",
        "        df = df.withColumn('Produktkomponent',lit(value))\\\n",
        "            .withColumn('MeterKey', lit(value))\\\n",
        "            .withColumn('Type',lit(value))\\\n",
        "            .withColumn('InstallationStation60KV',lit(value))\\\n",
        "            .select( 'Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset')\n",
        "        return df\n",
        "\n",
        "    elif len(df.columns)==18:\n",
        "\n",
        "        df = df.where(df.Type == 'FED')\n",
        "        return df\n",
        "    \n",
        "    else:\n",
        "        print(['Found columns: ', df.columns, \"Columns' number: \", len(df.columns), 'UNEXPECTED!',path ])\n",
        "        raise NameError('Columns number unexpected!')\n",
        "\n",
        "%pyspark\n",
        "\n",
        "# SKIP CORRUPTED FILES\n",
        "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
        "all_trefor_months = get_filePaths('hdfs dfs -ls /feddl/landing/trefor/quarter_hourly/*/*/')\n",
        "saving_folder = '/feddl/work/CDK/miriam/trefor/2109/0_formatting'\n",
        "num_of_paths = len(all_trefor_months)\n",
        "\n",
        "######################################### MONTH PATH INDEX\n",
        "for i in range(0,num_of_paths): \n",
        "\n",
        "    month_path = all_trefor_months[i]\n",
        "    print([i, month_path])\n",
        "    month_str = month_path[-2:]\n",
        "    year_str = month_path[-13:-9]\n",
        "    saving_path = saving_folder + f'/year={year_str}/month={month_str}'       \n",
        "    \n",
        "    daylyPaths = get_filePaths('hdfs dfs -ls ' + month_path +'/')\n",
        "    \n",
        "    ############################################## DAY PATH INDEX\n",
        "    for day_path in daylyPaths:\n",
        "    \n",
        "        df = spark.read.parquet(day_path)\n",
        "        # ADDING EMPTY COLUMNS WHEN NEEDED\n",
        "        df = fix_treforCols(df, day_path) \n",
        "        \n",
        "        # MARCH 2021 DATE BY DATE    \n",
        "        if ((year_str == '2021') and (month_str == '03')):\n",
        " \n",
        "            # CREATE LIST WITH THE RIGHT DATES\n",
        "            df_dates= df.select(col('Dato')).where((month(col('Dato'))==month_str) & (year(col('Dato'))==year_str)).distinct()\n",
        "            df_dates = df_dates.withColumn('Dato',col(\"Dato\").cast(StringType()) )\n",
        "            list_dates = df_dates.collect()\n",
        "        \n",
        "            # FILTER DF ON EACH DATE AND SAVE\n",
        "            if list_dates != []:\n",
        "                \n",
        "                ########################################################### DATES INDEX\n",
        "                for k in range(0,len(list_dates)):\n",
        "    \n",
        "                    # FILTER ON RIGHT DATES\n",
        "                    right_date = list_dates[k]['Dato']\n",
        "                    df_day = df.where(col('Dato') == right_date)\n",
        "                    \n",
        "                    # ADD DAY COLUMN\n",
        "                    df_day = df_day.withColumn('day', dayofmonth('Dato'))\n",
        "                    \n",
        "                    # SAVE DAY FILE\n",
        "                    df_day.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "            \n",
        "                    # DEBUGGING\n",
        "                    print(['FILE', day_path, 'DATE', right_date, 'SAVED', saving_path])\n",
        "        else:\n",
        " \n",
        "            # FILTER DATA WITH THE RIGHT DATES\n",
        "            df_month= df.where((month(col('Dato')) == month_str) & (year(col('Dato')) == year_str))\n",
        "            \n",
        "            # ADD DAY COLUMN\n",
        "            df_month = df_month.withColumn('day', dayofmonth('Dato'))\n",
        "            \n",
        "            # SAVE MONTH BY DATE\n",
        "            df_month.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "    \n",
        "            # DEBUGGING\n",
        "            print(['FILE', day_path, 'SAVED', saving_path])\n",
        "            \n",
        "    # FIND ALL EXTRA DATES\n",
        "    df_extraDates= df.select(col('Dato')).where((month(col('Dato'))!=month_str) | (year(col('Dato'))!=year_str)).distinct()\n",
        "    df_extraDates = df_extraDates.withColumn('Dato',col(\"Dato\").cast(StringType()) )\n",
        "    list_extraDates = df_extraDates.collect()\n",
        "    \n",
        "    # DEBUGGING\n",
        "    print(['Extra dates: ', list_extraDates])\n",
        "\n",
        "    # IF THERE ARE CORRECTIONS WRITE THEM IN THE RIGHT DATE-FOLDER\n",
        "    if list_extraDates != []:\n",
        "    \n",
        "        for j in range(0,len(list_extraDates)):\n",
        "\n",
        "            # FILTER ON CORRECTION DATES\n",
        "            extra_date = list_extraDates[j]['Dato']\n",
        "            df_correctionDay = df.where(col('Dato') == extra_date)\n",
        "        \n",
        "            # SAVE EXTRA DAY IN THE RIGHT DATE-PATH\n",
        "            year_correctionDay = extra_date[:4]\n",
        "            month_correctionDay = extra_date[-5:-3]\n",
        "\n",
        "            # ADD DAY COLUMN\n",
        "            df_correctionDay = df_correctionDay.withColumn('day', dayofmonth('Dato'))\n",
        "\n",
        "            # SAVE DATAFRAME AS PARQUET\n",
        "            saving_path = saving_folder + f'/year={year_correctionDay}/month={month_correctionDay}'\n",
        "            df_correctionDay.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "        \n",
        "            # DEBUGGING\n",
        "            print(['EXTRA DATE', extra_date, 'SAVED', saving_path])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}