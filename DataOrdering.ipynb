{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataOrdering",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/DataOrdering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "a0543c15-c6c4-40e2-a284-0adb52e8165b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIV8oOhQsnUA",
        "outputId": "47d8cdd6-c147-4c76-f540-3e7adf4cf6da"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_ConfigurationImportsFunctions.ipynb') # creates a local copy in the VM\n",
        "!jupyter nbconvert --to python 'Pyspark_ConfigurationImportsFunctions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_ConfigurationImportsFunctions.ipynb # deletes the local copy\n",
        "import Pyspark_ConfigurationImportsFunctions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Pyspark_ConfigurationImportsFunctions.ipynb to python\n",
            "[NbConvertApp] Writing 13668 bytes to Pyspark_ConfigurationImportsFunctions.py\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "/\n",
            "start file\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArrayType',\n",
              " 'BinaryType',\n",
              " 'BooleanType',\n",
              " 'ByteType',\n",
              " 'DataFrame',\n",
              " 'DataType',\n",
              " 'DateType',\n",
              " 'DecimalType',\n",
              " 'DoubleType',\n",
              " 'FloatType',\n",
              " 'IntegerType',\n",
              " 'LongType',\n",
              " 'MAXYEAR',\n",
              " 'MINYEAR',\n",
              " 'MapType',\n",
              " 'NullType',\n",
              " 'PandasUDFType',\n",
              " 'ShortType',\n",
              " 'SparkSession',\n",
              " 'StringType',\n",
              " 'StructField',\n",
              " 'StructType',\n",
              " 'TimestampType',\n",
              " 'UserDefinedFunction',\n",
              " 'WRAPPER_ASSIGNMENTS',\n",
              " 'WRAPPER_UPDATES',\n",
              " 'Window',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'abs',\n",
              " 'acos',\n",
              " 'add_index_column',\n",
              " 'add_months',\n",
              " 'approxCountDistinct',\n",
              " 'approx_count_distinct',\n",
              " 'array',\n",
              " 'array_contains',\n",
              " 'array_distinct',\n",
              " 'array_except',\n",
              " 'array_intersect',\n",
              " 'array_join',\n",
              " 'array_max',\n",
              " 'array_min',\n",
              " 'array_position',\n",
              " 'array_remove',\n",
              " 'array_repeat',\n",
              " 'array_sort',\n",
              " 'array_union',\n",
              " 'arrays_overlap',\n",
              " 'arrays_zip',\n",
              " 'asc',\n",
              " 'asc_nulls_first',\n",
              " 'asc_nulls_last',\n",
              " 'ascii',\n",
              " 'asin',\n",
              " 'atan',\n",
              " 'atan2',\n",
              " 'avg',\n",
              " 'base64',\n",
              " 'basestring',\n",
              " 'bin',\n",
              " 'bitwiseNOT',\n",
              " 'broadcast',\n",
              " 'bround',\n",
              " 'cbrt',\n",
              " 'ceil',\n",
              " 'chain',\n",
              " 'cmp_to_key',\n",
              " 'coalesce',\n",
              " 'col',\n",
              " 'col_to_hour',\n",
              " 'collect_list',\n",
              " 'collect_set',\n",
              " 'column',\n",
              " 'concat',\n",
              " 'concat_ws',\n",
              " 'conv',\n",
              " 'corr',\n",
              " 'cos',\n",
              " 'cosh',\n",
              " 'count',\n",
              " 'countDistinct',\n",
              " 'covar_pop',\n",
              " 'covar_samp',\n",
              " 'crc32',\n",
              " 'createStationCols',\n",
              " 'create_map',\n",
              " 'cume_dist',\n",
              " 'current_date',\n",
              " 'current_timestamp',\n",
              " 'date',\n",
              " 'date_add',\n",
              " 'date_format',\n",
              " 'date_sub',\n",
              " 'date_trunc',\n",
              " 'datediff',\n",
              " 'datetime',\n",
              " 'datetime_CAPI',\n",
              " 'dayofmonth',\n",
              " 'dayofweek',\n",
              " 'dayofyear',\n",
              " 'decode',\n",
              " 'degrees',\n",
              " 'dense_rank',\n",
              " 'desc',\n",
              " 'desc_nulls_first',\n",
              " 'desc_nulls_last',\n",
              " 'dfCol_to_time',\n",
              " 'df_filterByColumnValue',\n",
              " 'df_mapCol',\n",
              " 'df_to_dict',\n",
              " 'element_at',\n",
              " 'encode',\n",
              " 'exp',\n",
              " 'explode',\n",
              " 'explode_outer',\n",
              " 'expm1',\n",
              " 'expr',\n",
              " 'factorial',\n",
              " 'findspark',\n",
              " 'first',\n",
              " 'flatten',\n",
              " 'floor',\n",
              " 'format_number',\n",
              " 'format_string',\n",
              " 'forward_fill_int',\n",
              " 'forward_fill_str',\n",
              " 'from_csv',\n",
              " 'from_json',\n",
              " 'from_unixtime',\n",
              " 'from_utc_timestamp',\n",
              " 'get_filePaths',\n",
              " 'get_json_object',\n",
              " 'greatest',\n",
              " 'grouping',\n",
              " 'grouping_id',\n",
              " 'hash',\n",
              " 'hex',\n",
              " 'hour',\n",
              " 'hypot',\n",
              " 'initcap',\n",
              " 'input_file_name',\n",
              " 'instr',\n",
              " 'isfile',\n",
              " 'isnan',\n",
              " 'isnull',\n",
              " 'itertools',\n",
              " 'join',\n",
              " 'json_tuple',\n",
              " 'kurtosis',\n",
              " 'lag',\n",
              " 'last',\n",
              " 'last_day',\n",
              " 'lead',\n",
              " 'least',\n",
              " 'length',\n",
              " 'levenshtein',\n",
              " 'listdir',\n",
              " 'lit',\n",
              " 'locate',\n",
              " 'log',\n",
              " 'log10',\n",
              " 'log1p',\n",
              " 'log2',\n",
              " 'lower',\n",
              " 'lpad',\n",
              " 'lru_cache',\n",
              " 'ltrim',\n",
              " 'map_concat',\n",
              " 'map_entries',\n",
              " 'map_from_arrays',\n",
              " 'map_from_entries',\n",
              " 'map_keys',\n",
              " 'map_values',\n",
              " 'math',\n",
              " 'max',\n",
              " 'md5',\n",
              " 'mean',\n",
              " 'merge_cols',\n",
              " 'min',\n",
              " 'minute',\n",
              " 'monotonically_increasing_id',\n",
              " 'month',\n",
              " 'months_between',\n",
              " 'nanvl',\n",
              " 'next_day',\n",
              " 'ntile',\n",
              " 'os',\n",
              " 'overlay',\n",
              " 'pandas_udf',\n",
              " 'partial',\n",
              " 'partialmethod',\n",
              " 'percent_rank',\n",
              " 'posexplode',\n",
              " 'posexplode_outer',\n",
              " 'pow',\n",
              " 'quarter',\n",
              " 'radians',\n",
              " 'rand',\n",
              " 'randn',\n",
              " 'rank',\n",
              " 'read_timeseries',\n",
              " 'reduce',\n",
              " 'regexp_extract',\n",
              " 'regexp_replace',\n",
              " 'remove_cols',\n",
              " 'remove_file',\n",
              " 'rename_columns',\n",
              " 'repeat',\n",
              " 'reverse',\n",
              " 'rint',\n",
              " 'round',\n",
              " 'row_number',\n",
              " 'rpad',\n",
              " 'rtrim',\n",
              " 'schema_of_csv',\n",
              " 'schema_of_json',\n",
              " 'second',\n",
              " 'sequence',\n",
              " 'sha1',\n",
              " 'sha2',\n",
              " 'shiftLeft',\n",
              " 'shiftRight',\n",
              " 'shiftRightUnsigned',\n",
              " 'shuffle',\n",
              " 'signum',\n",
              " 'sin',\n",
              " 'singledispatch',\n",
              " 'sinh',\n",
              " 'size',\n",
              " 'skewness',\n",
              " 'slice',\n",
              " 'sort_array',\n",
              " 'soundex',\n",
              " 'spark',\n",
              " 'spark_partition_id',\n",
              " 'split',\n",
              " 'sqrt',\n",
              " 'stddev',\n",
              " 'stddev_pop',\n",
              " 'stddev_samp',\n",
              " 'struct',\n",
              " 'subprocess',\n",
              " 'substring',\n",
              " 'substring_index',\n",
              " 'sum',\n",
              " 'sumDistinct',\n",
              " 'sys',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'timezone',\n",
              " 'toDegrees',\n",
              " 'toRadians',\n",
              " 'to_csv',\n",
              " 'to_date',\n",
              " 'to_json',\n",
              " 'to_str',\n",
              " 'to_timestamp',\n",
              " 'to_utc_timestamp',\n",
              " 'total_ordering',\n",
              " 'translate',\n",
              " 'trim',\n",
              " 'trunc',\n",
              " 'tzinfo',\n",
              " 'udf',\n",
              " 'unbase64',\n",
              " 'unhex',\n",
              " 'unix_timestamp',\n",
              " 'update_wrapper',\n",
              " 'upper',\n",
              " 'var_pop',\n",
              " 'var_samp',\n",
              " 'variance',\n",
              " 'weekofyear',\n",
              " 'when',\n",
              " 'window',\n",
              " 'wraps',\n",
              " 'xxhash64',\n",
              " 'year']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIM3WUiPVChR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PoAow1Gro0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXiHiN4vKHXf"
      },
      "source": [
        "# **Ordering Time Series file in folders**\n",
        "\n",
        "> This script is meant to 'reorder' files to the right monthly and daily folder. The code process parquet data files, collected daily by meters since 4 years. The files are contained in month and day folders, in theese folders files with wrong timestamp are contained as well. The code process files relatively slow to avoid exceeding memory \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKdqYpqNKUmQ"
      },
      "source": [
        "# THE DATASET HAS COLUMNS THAT CHANGE OVER TIME\n",
        "# this function checks the number of columns and fix them adding missing columns with empty values\n",
        "'''\n",
        "['Dato', 'Time-Kvarter', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'ProduktkomponentType', 'Dataset', 'day'] ->\n",
        "['Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset', 'day']\n",
        "'''\n",
        "def fix_cols(df, path):\n",
        "    value = ''\n",
        "    if len(df.columns)==11:\n",
        "        \n",
        "        df = df.withColumn('Produktkomponent',lit(value))\\\n",
        "            .withColumn('MeterKey', lit(value))\\\n",
        "            .withColumn('InstallationKabelskab',lit(value))\\\n",
        "            .withColumn('InstallationLS_Udføring',lit(value))\\\n",
        "            .withColumn('InstallationNetstation',lit(value))\\\n",
        "            .withColumn('InstallationStation60KV',lit(value))\\\n",
        "            .withColumn('Type',lit(value))\\\n",
        "            .select( 'Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset')\n",
        "        return df\n",
        "    \n",
        "    elif len(df.columns)==14:\n",
        "\n",
        "        df = df.withColumn('Produktkomponent',lit(value))\\\n",
        "            .withColumn('MeterKey', lit(value))\\\n",
        "            .withColumn('Type',lit(value))\\\n",
        "            .withColumn('InstallationStation60KV',lit(value))\\\n",
        "            .select( 'Dato', 'Time-Kvarter', 'MeterKey', 'InstallationsID', 'InstallationAdresse', 'InstallationPostNr', 'MålerArtBeskrivelse', 'MålerEgenskab', 'MeterRegister', 'MålerVærdier', 'Produktkomponent', 'ProduktkomponentType', 'InstallationKabelskab', 'InstallationLS_Udføring', 'InstallationNetstation', 'InstallationStation60KV', 'Type', 'Dataset')\n",
        "        return df\n",
        "\n",
        "    elif len(df.columns)==18:\n",
        "\n",
        "        df = df.where(df.Type == 'FED')\n",
        "        return df\n",
        "    \n",
        "    else:\n",
        "        print(['Found columns: ', df.columns, \"Columns' number: \", len(df.columns), 'UNEXPECTED!',path ])\n",
        "        raise NameError('Columns number unexpected!')\n",
        "\n",
        "%pyspark\n",
        "\n",
        "# SKIP CORRUPTED FILES\n",
        "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
        "all_trefor_months = get_filePaths('hdfs dfs -ls /feddl/landing/trefor/quarter_hourly/*/*/')\n",
        "saving_folder = '/feddl/work/CDK/miriam/trefor/2109/0_formatting'\n",
        "num_of_paths = len(all_trefor_months)\n",
        "\n",
        "######################################### MONTH PATH INDEX\n",
        "for i in range(0,num_of_paths): \n",
        "\n",
        "    month_path = all_trefor_months[i]\n",
        "    print([i, month_path])\n",
        "    month_str = month_path[-2:]\n",
        "    year_str = month_path[-13:-9]\n",
        "    saving_path = saving_folder + f'/year={year_str}/month={month_str}'       \n",
        "    \n",
        "    daylyPaths = get_filePaths('hdfs dfs -ls ' + month_path +'/')\n",
        "    \n",
        "    ############################################## DAY PATH INDEX\n",
        "    for day_path in daylyPaths:\n",
        "    \n",
        "        df = spark.read.parquet(day_path)\n",
        "        # ADDING EMPTY COLUMNS WHEN NEEDED\n",
        "        df = fix_treforCols(df, day_path) \n",
        "        \n",
        "        # MARCH 2021 DATE BY DATE    \n",
        "        if ((year_str == '2021') and (month_str == '03')):\n",
        " \n",
        "            # CREATE LIST WITH THE RIGHT DATES\n",
        "            df_dates= df.select(col('Dato')).where((month(col('Dato'))==month_str) & (year(col('Dato'))==year_str)).distinct()\n",
        "            df_dates = df_dates.withColumn('Dato',col(\"Dato\").cast(StringType()) )\n",
        "            list_dates = df_dates.collect()\n",
        "        \n",
        "            # FILTER DF ON EACH DATE AND SAVE\n",
        "            if list_dates != []:\n",
        "                \n",
        "                ########################################################### DATES INDEX\n",
        "                for k in range(0,len(list_dates)):\n",
        "    \n",
        "                    # FILTER ON RIGHT DATES\n",
        "                    right_date = list_dates[k]['Dato']\n",
        "                    df_day = df.where(col('Dato') == right_date)\n",
        "                    \n",
        "                    # ADD DAY COLUMN\n",
        "                    df_day = df_day.withColumn('day', dayofmonth('Dato'))\n",
        "                    \n",
        "                    # SAVE DAY FILE\n",
        "                    df_day.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "            \n",
        "                    # DEBUGGING\n",
        "                    print(['FILE', day_path, 'DATE', right_date, 'SAVED', saving_path])\n",
        "        else:\n",
        " \n",
        "            # FILTER DATA WITH THE RIGHT DATES\n",
        "            df_month= df.where((month(col('Dato')) == month_str) & (year(col('Dato')) == year_str))\n",
        "            \n",
        "            # ADD DAY COLUMN\n",
        "            df_month = df_month.withColumn('day', dayofmonth('Dato'))\n",
        "            \n",
        "            # SAVE MONTH BY DATE\n",
        "            df_month.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "    \n",
        "            # DEBUGGING\n",
        "            print(['FILE', day_path, 'SAVED', saving_path])\n",
        "            \n",
        "    # FIND ALL EXTRA DATES\n",
        "    df_extraDates= df.select(col('Dato')).where((month(col('Dato'))!=month_str) | (year(col('Dato'))!=year_str)).distinct()\n",
        "    df_extraDates = df_extraDates.withColumn('Dato',col(\"Dato\").cast(StringType()) )\n",
        "    list_extraDates = df_extraDates.collect()\n",
        "    \n",
        "    # DEBUGGING\n",
        "    print(['Extra dates: ', list_extraDates])\n",
        "\n",
        "    # IF THERE ARE CORRECTIONS WRITE THEM IN THE RIGHT DATE-FOLDER\n",
        "    if list_extraDates != []:\n",
        "    \n",
        "        for j in range(0,len(list_extraDates)):\n",
        "\n",
        "            # FILTER ON CORRECTION DATES\n",
        "            extra_date = list_extraDates[j]['Dato']\n",
        "            df_correctionDay = df.where(col('Dato') == extra_date)\n",
        "        \n",
        "            # SAVE EXTRA DAY IN THE RIGHT DATE-PATH\n",
        "            year_correctionDay = extra_date[:4]\n",
        "            month_correctionDay = extra_date[-5:-3]\n",
        "\n",
        "            # ADD DAY COLUMN\n",
        "            df_correctionDay = df_correctionDay.withColumn('day', dayofmonth('Dato'))\n",
        "\n",
        "            # SAVE DATAFRAME AS PARQUET\n",
        "            saving_path = saving_folder + f'/year={year_correctionDay}/month={month_correctionDay}'\n",
        "            df_correctionDay.write.parquet(saving_path, mode='append', partitionBy='day')\n",
        "        \n",
        "            # DEBUGGING\n",
        "            print(['EXTRA DATE', extra_date, 'SAVED', saving_path])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}