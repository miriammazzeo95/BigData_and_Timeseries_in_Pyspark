{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "API_BBR",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/API_BBR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "19cf9b54-a565-425a-9882-3b631b03e64b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3V_q7gPUBTy"
      },
      "source": [
        " Import configuration Colab Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIV8oOhQsnUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0cc028-1660-4099-d0d4-695a92dc2b83"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# change directory to content\n",
        "%cd /\n",
        "%cd content\n",
        "\n",
        "# get sharinglink from Colab notebook to be imported\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_Configuration_Imports_Functions.ipynb') # creates a local copy in the local content folder\n",
        "!rm Pyspark_Configuration_Imports_Functions.py # deletes previouse version copy if any\n",
        "!jupyter nbconvert --to python 'Pyspark_Configuration_Imports_Functions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_Configuration_Imports_Functions.ipynb # deletes the local copy"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "/content\n",
            "[NbConvertApp] Converting notebook Pyspark_Configuration_Imports_Functions.ipynb to python\n",
            "[NbConvertApp] Writing 13117 bytes to Pyspark_Configuration_Imports_Functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZknbM5Kuvb5",
        "outputId": "b09c3ee6-d11b-4647-c364-35cde82e9723"
      },
      "source": [
        "ls "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adc.json                                    \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "\u001b[01;34mdrive\u001b[0m/                                      \u001b[01;34mspark-3.0.0-bin-hadoop3.2\u001b[0m/\n",
            "\u001b[01;34m__pycache__\u001b[0m/                                spark-3.0.0-bin-hadoop3.2.tgz\n",
            "Pyspark_Configuration_Imports_Functions.py  spark-3.0.0-bin-hadoop3.2.tgz.1\n",
            "Pyspark_ConfigurationImportsFunctions.py    spark-3.0.0-bin-hadoop3.2.tgz.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDYoAO6outPZ",
        "outputId": "34544d18-85ce-4924-f696-4db6e1d803c3"
      },
      "source": [
        "import Pyspark_Configuration_Imports_Functions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArrayType',\n",
              " 'BinaryType',\n",
              " 'BooleanType',\n",
              " 'ByteType',\n",
              " 'DataFrame',\n",
              " 'DataType',\n",
              " 'DateType',\n",
              " 'DecimalType',\n",
              " 'DoubleType',\n",
              " 'FloatType',\n",
              " 'IntegerType',\n",
              " 'LongType',\n",
              " 'MAXYEAR',\n",
              " 'MINYEAR',\n",
              " 'MapType',\n",
              " 'NullType',\n",
              " 'PandasUDFType',\n",
              " 'ShortType',\n",
              " 'SparkSession',\n",
              " 'StringType',\n",
              " 'StructField',\n",
              " 'StructType',\n",
              " 'TimestampType',\n",
              " 'UserDefinedFunction',\n",
              " 'WRAPPER_ASSIGNMENTS',\n",
              " 'WRAPPER_UPDATES',\n",
              " 'Window',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'abs',\n",
              " 'acos',\n",
              " 'add_index_column',\n",
              " 'add_months',\n",
              " 'approxCountDistinct',\n",
              " 'approx_count_distinct',\n",
              " 'array',\n",
              " 'array_contains',\n",
              " 'array_distinct',\n",
              " 'array_except',\n",
              " 'array_intersect',\n",
              " 'array_join',\n",
              " 'array_max',\n",
              " 'array_min',\n",
              " 'array_position',\n",
              " 'array_remove',\n",
              " 'array_repeat',\n",
              " 'array_sort',\n",
              " 'array_union',\n",
              " 'arrays_overlap',\n",
              " 'arrays_zip',\n",
              " 'asc',\n",
              " 'asc_nulls_first',\n",
              " 'asc_nulls_last',\n",
              " 'ascii',\n",
              " 'asin',\n",
              " 'atan',\n",
              " 'atan2',\n",
              " 'avg',\n",
              " 'base64',\n",
              " 'basestring',\n",
              " 'bin',\n",
              " 'bitwiseNOT',\n",
              " 'broadcast',\n",
              " 'bround',\n",
              " 'cbrt',\n",
              " 'ceil',\n",
              " 'chain',\n",
              " 'cmp_to_key',\n",
              " 'coalesce',\n",
              " 'col',\n",
              " 'col_to_hour',\n",
              " 'collect_list',\n",
              " 'collect_set',\n",
              " 'column',\n",
              " 'concat',\n",
              " 'concat_ws',\n",
              " 'conv',\n",
              " 'corr',\n",
              " 'cos',\n",
              " 'cosh',\n",
              " 'count',\n",
              " 'countDistinct',\n",
              " 'covar_pop',\n",
              " 'covar_samp',\n",
              " 'crc32',\n",
              " 'createStationCols',\n",
              " 'create_map',\n",
              " 'cume_dist',\n",
              " 'current_date',\n",
              " 'current_timestamp',\n",
              " 'date',\n",
              " 'date_add',\n",
              " 'date_format',\n",
              " 'date_sub',\n",
              " 'date_trunc',\n",
              " 'datediff',\n",
              " 'datetime',\n",
              " 'datetime_CAPI',\n",
              " 'dayofmonth',\n",
              " 'dayofweek',\n",
              " 'dayofyear',\n",
              " 'decode',\n",
              " 'degrees',\n",
              " 'dense_rank',\n",
              " 'desc',\n",
              " 'desc_nulls_first',\n",
              " 'desc_nulls_last',\n",
              " 'dfCol_to_time',\n",
              " 'df_filterByColumnValue',\n",
              " 'df_mapCol',\n",
              " 'df_to_dict',\n",
              " 'element_at',\n",
              " 'encode',\n",
              " 'exp',\n",
              " 'explode',\n",
              " 'explode_outer',\n",
              " 'expm1',\n",
              " 'expr',\n",
              " 'factorial',\n",
              " 'findspark',\n",
              " 'first',\n",
              " 'flatten',\n",
              " 'floor',\n",
              " 'format_number',\n",
              " 'format_string',\n",
              " 'forward_fill_int',\n",
              " 'forward_fill_str',\n",
              " 'from_csv',\n",
              " 'from_json',\n",
              " 'from_unixtime',\n",
              " 'from_utc_timestamp',\n",
              " 'get_filePaths',\n",
              " 'get_json_object',\n",
              " 'greatest',\n",
              " 'grouping',\n",
              " 'grouping_id',\n",
              " 'hash',\n",
              " 'hex',\n",
              " 'hour',\n",
              " 'hypot',\n",
              " 'initcap',\n",
              " 'input_file_name',\n",
              " 'instr',\n",
              " 'isfile',\n",
              " 'isnan',\n",
              " 'isnull',\n",
              " 'itertools',\n",
              " 'join',\n",
              " 'json_tuple',\n",
              " 'kurtosis',\n",
              " 'lag',\n",
              " 'last',\n",
              " 'last_day',\n",
              " 'lead',\n",
              " 'least',\n",
              " 'length',\n",
              " 'levenshtein',\n",
              " 'listdir',\n",
              " 'lit',\n",
              " 'locate',\n",
              " 'log',\n",
              " 'log10',\n",
              " 'log1p',\n",
              " 'log2',\n",
              " 'lower',\n",
              " 'lpad',\n",
              " 'lru_cache',\n",
              " 'ltrim',\n",
              " 'map_concat',\n",
              " 'map_entries',\n",
              " 'map_from_arrays',\n",
              " 'map_from_entries',\n",
              " 'map_keys',\n",
              " 'map_values',\n",
              " 'math',\n",
              " 'max',\n",
              " 'md5',\n",
              " 'mean',\n",
              " 'merge_cols',\n",
              " 'min',\n",
              " 'minute',\n",
              " 'monotonically_increasing_id',\n",
              " 'month',\n",
              " 'months_between',\n",
              " 'nanvl',\n",
              " 'next_day',\n",
              " 'ntile',\n",
              " 'os',\n",
              " 'overlay',\n",
              " 'pandas_udf',\n",
              " 'partial',\n",
              " 'partialmethod',\n",
              " 'percent_rank',\n",
              " 'posexplode',\n",
              " 'posexplode_outer',\n",
              " 'pow',\n",
              " 'quarter',\n",
              " 'radians',\n",
              " 'rand',\n",
              " 'randn',\n",
              " 'rank',\n",
              " 'read_timeseries',\n",
              " 'reduce',\n",
              " 'regexp_extract',\n",
              " 'regexp_replace',\n",
              " 'remove_cols',\n",
              " 'remove_file',\n",
              " 'rename_columns',\n",
              " 'repeat',\n",
              " 'reverse',\n",
              " 'rint',\n",
              " 'round',\n",
              " 'row_number',\n",
              " 'rpad',\n",
              " 'rtrim',\n",
              " 'schema_of_csv',\n",
              " 'schema_of_json',\n",
              " 'second',\n",
              " 'sequence',\n",
              " 'sha1',\n",
              " 'sha2',\n",
              " 'shiftLeft',\n",
              " 'shiftRight',\n",
              " 'shiftRightUnsigned',\n",
              " 'shuffle',\n",
              " 'signum',\n",
              " 'sin',\n",
              " 'singledispatch',\n",
              " 'sinh',\n",
              " 'size',\n",
              " 'skewness',\n",
              " 'slice',\n",
              " 'sort_array',\n",
              " 'soundex',\n",
              " 'spark',\n",
              " 'spark_partition_id',\n",
              " 'split',\n",
              " 'sqrt',\n",
              " 'stddev',\n",
              " 'stddev_pop',\n",
              " 'stddev_samp',\n",
              " 'struct',\n",
              " 'subprocess',\n",
              " 'substring',\n",
              " 'substring_index',\n",
              " 'sum',\n",
              " 'sumDistinct',\n",
              " 'sys',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'timezone',\n",
              " 'toDegrees',\n",
              " 'toRadians',\n",
              " 'to_csv',\n",
              " 'to_date',\n",
              " 'to_json',\n",
              " 'to_str',\n",
              " 'to_timestamp',\n",
              " 'to_utc_timestamp',\n",
              " 'total_ordering',\n",
              " 'translate',\n",
              " 'trim',\n",
              " 'trunc',\n",
              " 'tzinfo',\n",
              " 'udf',\n",
              " 'unbase64',\n",
              " 'unhex',\n",
              " 'unix_timestamp',\n",
              " 'update_wrapper',\n",
              " 'upper',\n",
              " 'var_pop',\n",
              " 'var_samp',\n",
              " 'variance',\n",
              " 'weekofyear',\n",
              " 'when',\n",
              " 'window',\n",
              " 'wraps',\n",
              " 'xxhash64',\n",
              " 'year']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTd3ZtN6x61G"
      },
      "source": [
        "# **Building Register**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASS6hlzMrMiO"
      },
      "source": [
        "# **Define BBR Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ETXszdDr63w"
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### === BBR === ###\n",
        "class BBR:\n",
        "    '''\n",
        "    Class to collect information about addresses from BBR (Bygnings- og Boligregistret)\n",
        "\n",
        "    Functions:\n",
        "        collect_items():\n",
        "            Uses the helping functions grab_id and collect_coords to pull information from the BBR API and\n",
        "            create a pandas Dataframe with the information.\n",
        "        collect_coords():\n",
        "            Collects the coordinates of the specified address from the BBR API.\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, addresses):\n",
        "        # Address URL\n",
        "        self.ad_url = \"https://dawa.aws.dk/adresser?q=\"\n",
        "        # ID URL\n",
        "        self.id_url = \"https://dawa.aws.dk/bbrlight/enheder?adresseid=\"\n",
        "        # BBR Items\n",
        "        self.items = ['BYG_ANVEND_KODE', 'BYG_ARL_SAML', 'BYG_BOLIG_ARL_SAML', 'BYG_BEBYG_ARL', 'ERHV_ARL_SAML', 'GARAGE_INDB_ARL', 'CARPORT_INDB_ARL', 'UDHUS_INDB_ARL', 'UDESTUE_ARL',\n",
        "                      'OPFOERELSE_AAR', 'OMBYG_AAR', 'VARMEINSTAL_KODE', 'OPVARMNING_KODE', 'VARME_SUPPL_KODE', 'BYG_VANDFORSY_KODE', 'ETAGER_ANT', 'ETAGER_AFVIG_KODE', 'YDERVAEG_KODE', 'TAG_KODE']\n",
        "        # Addresses parsed\n",
        "        self.adds = addresses\n",
        "\n",
        "    # get the address id\n",
        "\n",
        "    def grab_id(self, address):\n",
        "        try:\n",
        "            res = requests.get(self.ad_url+address)\n",
        "            raw_addr = json.loads(res.content)\n",
        "            _id = raw_addr[0]['id']\n",
        "            return _id\n",
        "        except IndexError as e:\n",
        "            return ''\n",
        "\n",
        "    # get the coordinates\n",
        "\n",
        "    def collect_coords(self, address):\n",
        "        try:\n",
        "            res = requests.get(self.ad_url+address)\n",
        "            raw_addr = json.loads(res.content)\n",
        "            lon = raw_addr[0]['adgangsadresse']['adgangspunkt']['koordinater'][0]\n",
        "            lat = raw_addr[0]['adgangsadresse']['adgangspunkt']['koordinater'][1]\n",
        "            return [lon, lat]\n",
        "        except IndexError as e:\n",
        "            return '', ''\n",
        "\n",
        "    # Collect items specified from BBR\n",
        "\n",
        "    def collect_items(self):\n",
        "        ''' Uses a list of addresses and a list of items to be pulled from\n",
        "        BBR and creates a dataframe with these'''\n",
        "\n",
        "        root = 'bygning'\n",
        "        data = defaultdict(list)\n",
        "\n",
        "        for address in self.adds:\n",
        "            rsp = requests.get(self.id_url + self.grab_id(address))\n",
        "            raw = json.loads(rsp.content)\n",
        "            try:\n",
        "                data['Address'].append(address)\n",
        "                data['Lat'].append(self.collect_coords(address)[1])\n",
        "                data['Lon'].append(self.collect_coords(address)[0])\n",
        "                for item in self.items:\n",
        "                    res = raw[0][root][item]\n",
        "                    data[item].append(res)\n",
        "            except (IndexError, KeyError) as e:\n",
        "                for item in self.items:\n",
        "                    data[item].append(np.nan)\n",
        "        df = pd.DataFrame.from_dict(data)\n",
        "        return df\n",
        "        "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUH5ih-8r8NQ"
      },
      "source": [
        "# FOR CONVERTING PANDAS-DF IN OUTPUT FROM BBR FUNCTION TO SPARK DF        \n",
        "def BBR_fix_dtypes(df):\n",
        "    df = df.fillna('')\n",
        "    df = df.infer_objects()\n",
        "    df = df.astype({'Address':'string'})\n",
        "    df = df.astype({'VARMEINSTAL_KODE':'int64',  'OPVARMNING_KODE':'string', 'VARME_SUPPL_KODE':'string', 'YDERVAEG_KODE':'int64', 'TAG_KODE':'int64'}, errors='ignore')\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs78PLNLx0iT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}