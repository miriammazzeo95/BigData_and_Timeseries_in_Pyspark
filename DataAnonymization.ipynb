{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataAnonymization",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/DataAnonymization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "a0543c15-c6c4-40e2-a284-0adb52e8165b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3V_q7gPUBTy"
      },
      "source": [
        " Import configuration Colab Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIV8oOhQsnUA"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_ConfigurationImportsFunctions.ipynb') # creates a local copy in the VM\n",
        "!jupyter nbconvert --to python 'Pyspark_ConfigurationImportsFunctions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_ConfigurationImportsFunctions.ipynb # deletes the local copy\n",
        "import Pyspark_ConfigurationImportsFunctions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PoAow1Gro0"
      },
      "source": [
        "# **Anonymize data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwJs8eCgbITG"
      },
      "source": [
        "**Dictionary** \n",
        "> We want to aggregate data from multiple addresses on Commune level\n",
        "\n",
        "> We thus need to map addresses to corresponding Power stations to aggregate data on those Power stations\n",
        "\n",
        "> Going through all monthly meter-data, we create a dictionary [Adress --> StationID] of unique Address/ID couples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW0t3Zu8b8O5"
      },
      "source": [
        "# WE CREATE A DICTIONARY [Adress --> StationID] STARTING READING FROM APRIL 2021\n",
        "# THE FIRST STATION-ID COLUMN GET INTRODUCED IN MARCH THE 3rd 2021\n",
        "\n",
        "# GET PATHS TO RAW DATA MONTH-FOLDERS IN THE YEAR 2020 IN HDFS DATA LAKE\n",
        "month_paths = get_filePaths('hdfs dfs -ls /feddl/landing/ENERGYDATA/quarter_hourly/daily/year=2021/')\n",
        "\n",
        "# READING 1 MONTH (APRIL 2021) OF RAW DATA IN HDFS DATA LAKE\n",
        "df = spark.read.parquet(month_paths[3])\n",
        "# SELECT COLUMNS\n",
        "df = df.select('InstallationAdresse','PostNr', 'PowerStationID')\n",
        "# MERGE COLUMNS AND DROP DUPLICATES\n",
        "df_union = merge_cols(df, 'InstallationAdresse', 'PostNr', 'InstallationAdresse', sep=' ').dropDuplicates(['InstallationAdresse']) \n",
        "\n",
        "i=4\n",
        "print(len(month_paths[i:]))\n",
        "\n",
        "# WE READ ONE MONTH AT A TIME, SELECT ADDRESS+POST-NUM AND STATION-ID\n",
        "# WE DROP DUPLICATES AND MERGE THE NEW COUPLE ADDRESS-ID TO THE DICTIONARY-DF INITIALISED ABOVE df_union\n",
        "for path in month_paths[i:]:\n",
        "    \n",
        "    df = spark.read.parquet(path)\n",
        "    df = df.select('InstallationAdresse','PostNr', 'PowerStationID')\n",
        "    df = merge_cols(df, 'InstallationAdresse', 'PostNr', 'InstallationAdresse', sep=' ')\n",
        "    df = df.dropDuplicates(['InstallationAdresse']) \n",
        "    df_union = df.unionAll(df).dropDuplicates(['InstallationAdresse'])\n",
        "    \n",
        "    print([i, path])\n",
        "    i += 1\n",
        "\n",
        "\n",
        "df_union.write.parquet('/feddl/work/miriam/ENERGYDATA/address_powerStation')\n",
        "\n",
        "# IF THERE ARE NULL VALUES IN THE ADDRESSES COLUMN\n",
        "# df_union = df_union.where(col('InstallationAdresse').isNotNull() & (col('InstallationAdresse')!='\\?'))\n",
        "\n",
        "# IF THERE ARE NULL VALUES IN THE STATION-ID COLUMN -> FORWARD FILL\n",
        "  \n",
        "# TO COUNT THE TOTAL ADDRESSES-ID COUPLE FOUND\n",
        "# df_union.count() # 139936\n",
        "# spark.read.parquet('/feddl/work/miriam/ENERGYDATA/address_powerStation').count() # 139936\n",
        "\n",
        "# TAKEN AROUND 10 MIN\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3-0M6z8apIP"
      },
      "source": [
        "**Anonymization**\n",
        "> In order to display data we need to use Anonymization techniques\n",
        "\n",
        "> Using the created dictionary Adress->InstallationID, we map a column in all the historical data containing power-station-IDs \n",
        "\n",
        "> The adress column and the dictionary gets deleted to be GDPR complient\n",
        "\n",
        "> Future step would be remapping those IDs to random generated numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atgQeXuGap9T"
      },
      "source": [
        "# SOME USEFUL FINCTIONS\n",
        "\n",
        "# CONVERT SPARK DF OF 2 COLUMNS TO DICTIONARY\n",
        "def df_to_dict(path):\n",
        "    # READ DF-DICTIONARY (SAVED PREVIOUSLY) IN 'feddl/work/miriam/ENERGYDATA/address_powerStation/'\n",
        "    df_dict = spark.read.parquet(path)\n",
        "    #df_dict.explain(True)\n",
        "    # CONVERT STRING COLUMN TO INTEGER\n",
        "    df_dict = df_dict.withColumn(\"PowerStationID\", col('PowerStationID').cast('int'))\n",
        "    # SORTING ALPHABETICALLY\n",
        "    df_dict = df_dict.sort(col(\"InstallationAdresse\").asc())\n",
        "    # CONVERT TO DICTIONARY \n",
        "    dictionary = {row['InstallationAdresse']:row['PowerStationID'] for row in df_dict.collect()}\n",
        "    return dictionary\n",
        "\n",
        "# ADD A COLUMN WITH THE SAME VALUE\n",
        "def add_commodity(df, t: str):\n",
        "    df = df.withColumn('Commodity', lit(t))\n",
        "    return df\n",
        "\n",
        "# CONVERT DATE AND TIME COLUMNS TO 1 TIMESTAMP COLUMN\n",
        "def get_timestamp(df):\n",
        "    df = merge_cols(df, 'Dato', 'Hour', 'TimeStamp', sep=' ')\n",
        "    df = df.withColumn('TimeStamp', to_timestamp(col(\"TimeStamp\"), 'yyyy-MM-dd HH:mm:ss'))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT4IhXfIlB6y"
      },
      "source": [
        "address_dictionary = df_to_dict('/feddl/work/miriam/ENERGYDATA/address_kabelskab')\n",
        "\n",
        "cleaning_monthsPaths = get_filePaths( 'hdfs dfs -ls /feddl/work/miriam/ENERGYDATA/1_cleaning/*/*/')\n",
        "saving_folder = '/feddl/work/miriam/ENERGYDATA/2_anonymization'\n",
        "test_folder = '/feddl/work/miriam/test/ENERGYDATA/2_anonymization'\n",
        "\n",
        "num_paths = len(cleaning_monthsPaths)\n",
        "#print(cleaning_monthsPaths)\n",
        "\n",
        "for p in range(0,num_paths):\n",
        "    \n",
        "    # GET MONTH_PATH\n",
        "    path = cleaning_monthsPaths[p]\n",
        "    print(path)\n",
        "    \n",
        "    # READ FILE\n",
        "    df = spark.read.parquet(path)\n",
        "    \n",
        "    # MAP 1 COLUMN PowerStationID (MISSING IN DATA BEFORE MARCH 2020)\n",
        "    df = df_mapCol(df, address_dictionary, 'PowerStationID', 'AdDress')\n",
        "    \n",
        "    # GET TIMESTAMP COLUMN\n",
        "    df = get_timestamp(df)\n",
        "    df = df.withColumn('day', col('day').cast(StringType()))\n",
        "    \n",
        "    # AGGREGATION OF Value ON PowerStationID - ANONIMIZATION\n",
        "    df = df.groupBy('TimeStamp', 'day', 'Measurement', 'PowerStationID').sum()\n",
        "    df = df.withColumnRenamed(\"sum(Value)\",\"Value\")\n",
        "    \n",
        "    # ADD DAY COLUMN\n",
        "    #df = df.withColumn('day', dayofmonth('Dato'))\n",
        "    \n",
        "    # GET COMMODITY TYPE: electricity, heat, water\n",
        "    df = df.withColumn('Commodity', when((col('Measurement') == 'EL_A_kWh') | (col('Measurement') == 'EL_E_kWh') | (col('Measurement') == 'P_A_kWh') | (col('Measurement') == 'P_E_KWh'), 'electricity')\n",
        "        .when( (col('Measurement')== 'EL_Va_m3') | (col('Measurement') == 'W_M2v2_m3') | (col('Measurement') == 'W_M2v1_m3') | (col('Measurement') == 'W_cM2v1_m3') | (col('Measurement') == 'W_cM2v2_m3'), 'water')\n",
        "        .when( (col('Measurement')== 'EL_Fv_m3') | (col('Measurement') == 'H_M1v4_C') | (col('Measurement') == 'H_M3v4_m3'), 'heat'))\n",
        "\n",
        "    # DROP ROWS WHERE PowerStationID IS EMPTY\n",
        "    df = df.where((col('PowerStationID') != '?') & (col('PowerStationID').isNotNull()))\n",
        "    \n",
        "    # CREATE SAVING PATH\n",
        "    chrono_folder = path.split('g')[-1]\n",
        "    saving_path = saving_folder + chrono_folder\n",
        "    \n",
        "    # SAVE MONTH BY DATE\n",
        "    df.write.parquet(saving_path, partitionBy='day')\n",
        "    \n",
        "    # DEBUGGING\n",
        "    print([p,'SAVED', saving_path])\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}