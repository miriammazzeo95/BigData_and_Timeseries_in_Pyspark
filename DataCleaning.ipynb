{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataCleaning",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "a0543c15-c6c4-40e2-a284-0adb52e8165b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIV8oOhQsnUA",
        "outputId": "47d8cdd6-c147-4c76-f540-3e7adf4cf6da"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_ConfigurationImportsFunctions.ipynb') # creates a local copy in the VM\n",
        "!jupyter nbconvert --to python 'Pyspark_ConfigurationImportsFunctions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_ConfigurationImportsFunctions.ipynb # deletes the local copy\n",
        "import Pyspark_ConfigurationImportsFunctions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook Pyspark_ConfigurationImportsFunctions.ipynb to python\n",
            "[NbConvertApp] Writing 13668 bytes to Pyspark_ConfigurationImportsFunctions.py\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "/\n",
            "start file\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArrayType',\n",
              " 'BinaryType',\n",
              " 'BooleanType',\n",
              " 'ByteType',\n",
              " 'DataFrame',\n",
              " 'DataType',\n",
              " 'DateType',\n",
              " 'DecimalType',\n",
              " 'DoubleType',\n",
              " 'FloatType',\n",
              " 'IntegerType',\n",
              " 'LongType',\n",
              " 'MAXYEAR',\n",
              " 'MINYEAR',\n",
              " 'MapType',\n",
              " 'NullType',\n",
              " 'PandasUDFType',\n",
              " 'ShortType',\n",
              " 'SparkSession',\n",
              " 'StringType',\n",
              " 'StructField',\n",
              " 'StructType',\n",
              " 'TimestampType',\n",
              " 'UserDefinedFunction',\n",
              " 'WRAPPER_ASSIGNMENTS',\n",
              " 'WRAPPER_UPDATES',\n",
              " 'Window',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'abs',\n",
              " 'acos',\n",
              " 'add_index_column',\n",
              " 'add_months',\n",
              " 'approxCountDistinct',\n",
              " 'approx_count_distinct',\n",
              " 'array',\n",
              " 'array_contains',\n",
              " 'array_distinct',\n",
              " 'array_except',\n",
              " 'array_intersect',\n",
              " 'array_join',\n",
              " 'array_max',\n",
              " 'array_min',\n",
              " 'array_position',\n",
              " 'array_remove',\n",
              " 'array_repeat',\n",
              " 'array_sort',\n",
              " 'array_union',\n",
              " 'arrays_overlap',\n",
              " 'arrays_zip',\n",
              " 'asc',\n",
              " 'asc_nulls_first',\n",
              " 'asc_nulls_last',\n",
              " 'ascii',\n",
              " 'asin',\n",
              " 'atan',\n",
              " 'atan2',\n",
              " 'avg',\n",
              " 'base64',\n",
              " 'basestring',\n",
              " 'bin',\n",
              " 'bitwiseNOT',\n",
              " 'broadcast',\n",
              " 'bround',\n",
              " 'cbrt',\n",
              " 'ceil',\n",
              " 'chain',\n",
              " 'cmp_to_key',\n",
              " 'coalesce',\n",
              " 'col',\n",
              " 'col_to_hour',\n",
              " 'collect_list',\n",
              " 'collect_set',\n",
              " 'column',\n",
              " 'concat',\n",
              " 'concat_ws',\n",
              " 'conv',\n",
              " 'corr',\n",
              " 'cos',\n",
              " 'cosh',\n",
              " 'count',\n",
              " 'countDistinct',\n",
              " 'covar_pop',\n",
              " 'covar_samp',\n",
              " 'crc32',\n",
              " 'createStationCols',\n",
              " 'create_map',\n",
              " 'cume_dist',\n",
              " 'current_date',\n",
              " 'current_timestamp',\n",
              " 'date',\n",
              " 'date_add',\n",
              " 'date_format',\n",
              " 'date_sub',\n",
              " 'date_trunc',\n",
              " 'datediff',\n",
              " 'datetime',\n",
              " 'datetime_CAPI',\n",
              " 'dayofmonth',\n",
              " 'dayofweek',\n",
              " 'dayofyear',\n",
              " 'decode',\n",
              " 'degrees',\n",
              " 'dense_rank',\n",
              " 'desc',\n",
              " 'desc_nulls_first',\n",
              " 'desc_nulls_last',\n",
              " 'dfCol_to_time',\n",
              " 'df_filterByColumnValue',\n",
              " 'df_mapCol',\n",
              " 'df_to_dict',\n",
              " 'element_at',\n",
              " 'encode',\n",
              " 'exp',\n",
              " 'explode',\n",
              " 'explode_outer',\n",
              " 'expm1',\n",
              " 'expr',\n",
              " 'factorial',\n",
              " 'findspark',\n",
              " 'first',\n",
              " 'flatten',\n",
              " 'floor',\n",
              " 'format_number',\n",
              " 'format_string',\n",
              " 'forward_fill_int',\n",
              " 'forward_fill_str',\n",
              " 'from_csv',\n",
              " 'from_json',\n",
              " 'from_unixtime',\n",
              " 'from_utc_timestamp',\n",
              " 'get_filePaths',\n",
              " 'get_json_object',\n",
              " 'greatest',\n",
              " 'grouping',\n",
              " 'grouping_id',\n",
              " 'hash',\n",
              " 'hex',\n",
              " 'hour',\n",
              " 'hypot',\n",
              " 'initcap',\n",
              " 'input_file_name',\n",
              " 'instr',\n",
              " 'isfile',\n",
              " 'isnan',\n",
              " 'isnull',\n",
              " 'itertools',\n",
              " 'join',\n",
              " 'json_tuple',\n",
              " 'kurtosis',\n",
              " 'lag',\n",
              " 'last',\n",
              " 'last_day',\n",
              " 'lead',\n",
              " 'least',\n",
              " 'length',\n",
              " 'levenshtein',\n",
              " 'listdir',\n",
              " 'lit',\n",
              " 'locate',\n",
              " 'log',\n",
              " 'log10',\n",
              " 'log1p',\n",
              " 'log2',\n",
              " 'lower',\n",
              " 'lpad',\n",
              " 'lru_cache',\n",
              " 'ltrim',\n",
              " 'map_concat',\n",
              " 'map_entries',\n",
              " 'map_from_arrays',\n",
              " 'map_from_entries',\n",
              " 'map_keys',\n",
              " 'map_values',\n",
              " 'math',\n",
              " 'max',\n",
              " 'md5',\n",
              " 'mean',\n",
              " 'merge_cols',\n",
              " 'min',\n",
              " 'minute',\n",
              " 'monotonically_increasing_id',\n",
              " 'month',\n",
              " 'months_between',\n",
              " 'nanvl',\n",
              " 'next_day',\n",
              " 'ntile',\n",
              " 'os',\n",
              " 'overlay',\n",
              " 'pandas_udf',\n",
              " 'partial',\n",
              " 'partialmethod',\n",
              " 'percent_rank',\n",
              " 'posexplode',\n",
              " 'posexplode_outer',\n",
              " 'pow',\n",
              " 'quarter',\n",
              " 'radians',\n",
              " 'rand',\n",
              " 'randn',\n",
              " 'rank',\n",
              " 'read_timeseries',\n",
              " 'reduce',\n",
              " 'regexp_extract',\n",
              " 'regexp_replace',\n",
              " 'remove_cols',\n",
              " 'remove_file',\n",
              " 'rename_columns',\n",
              " 'repeat',\n",
              " 'reverse',\n",
              " 'rint',\n",
              " 'round',\n",
              " 'row_number',\n",
              " 'rpad',\n",
              " 'rtrim',\n",
              " 'schema_of_csv',\n",
              " 'schema_of_json',\n",
              " 'second',\n",
              " 'sequence',\n",
              " 'sha1',\n",
              " 'sha2',\n",
              " 'shiftLeft',\n",
              " 'shiftRight',\n",
              " 'shiftRightUnsigned',\n",
              " 'shuffle',\n",
              " 'signum',\n",
              " 'sin',\n",
              " 'singledispatch',\n",
              " 'sinh',\n",
              " 'size',\n",
              " 'skewness',\n",
              " 'slice',\n",
              " 'sort_array',\n",
              " 'soundex',\n",
              " 'spark',\n",
              " 'spark_partition_id',\n",
              " 'split',\n",
              " 'sqrt',\n",
              " 'stddev',\n",
              " 'stddev_pop',\n",
              " 'stddev_samp',\n",
              " 'struct',\n",
              " 'subprocess',\n",
              " 'substring',\n",
              " 'substring_index',\n",
              " 'sum',\n",
              " 'sumDistinct',\n",
              " 'sys',\n",
              " 'tan',\n",
              " 'tanh',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'timezone',\n",
              " 'toDegrees',\n",
              " 'toRadians',\n",
              " 'to_csv',\n",
              " 'to_date',\n",
              " 'to_json',\n",
              " 'to_str',\n",
              " 'to_timestamp',\n",
              " 'to_utc_timestamp',\n",
              " 'total_ordering',\n",
              " 'translate',\n",
              " 'trim',\n",
              " 'trunc',\n",
              " 'tzinfo',\n",
              " 'udf',\n",
              " 'unbase64',\n",
              " 'unhex',\n",
              " 'unix_timestamp',\n",
              " 'update_wrapper',\n",
              " 'upper',\n",
              " 'var_pop',\n",
              " 'var_samp',\n",
              " 'variance',\n",
              " 'weekofyear',\n",
              " 'when',\n",
              " 'window',\n",
              " 'wraps',\n",
              " 'xxhash64',\n",
              " 'year']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIM3WUiPVChR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PoAow1Gro0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXiHiN4vKHXf"
      },
      "source": [
        "# **Ordering Time Series file in folders**\n",
        "\n",
        "> This script is meant to 'reorder' files to the right monthly and daily folder. The code process parquet data files, collected daily by meters since 4 years. The files are contained in month and day folders, in theese folders files with wrong timestamp are contained as well. The code process files relatively slow to avoid exceeding memory \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cEGYWlyO4iX"
      },
      "source": [
        "def clean_agg(df, ff=True):\n",
        "    \n",
        "    # UNCOMMENT WHENEVER THERE ARE EMPTY PARQUET FILE\n",
        "    #if df.first() == None:\n",
        "    #    remove_file(p)\n",
        "    #    print(f'REMOVED {p} ')\n",
        "    #    return df\n",
        "    #else:\n",
        "    # ...\n",
        "    \n",
        "    # DROP COLS\n",
        "    df=df.drop('MeasuredSpecies', 'MeasuredProperty', 'MeterRegister') \n",
        "    if ff:\n",
        "        # FORWARD FILL (NUMERICAL VALUES)\n",
        "        df = forward_fill_int(df, 'MeterValue',  ['Measurement','Time-Quarter'], 'MeterValue')\n",
        "    # AGGREGATION OF MeterValue ON HOUR (keeping same DATETIME, MEASUREMENT AND COMPONENT TYPES, ADDRESS)\n",
        "    df = df.withColumnRenamed(\"ProductcomponentType\",\"Component\")\n",
        "    df = df.groupBy('Dato', 'Hour',  'InstallationAdresse', 'Measurement', 'Component').sum()\n",
        "    df = df.withColumnRenamed(\"sum(MeterValue)\",\"Value\")\n",
        "    # DROP COLUMN\n",
        "    df = df.drop(col(\"Time-Quarter\"))\n",
        "    # ADD DAY COLUMN IF NECESSARY\n",
        "    #df = df.withColumn('day', dayofmonth('Dato'))\n",
        "    return df\n",
        "\n",
        "## SET UP READING PATH FROM HDFS - WHERE DATAFRAMES ARE STORED IN PARQUET\n",
        "# dataframes are stored in year=../month=../day=.. folders\n",
        "reading_folder = '/feddl/work/miriam/orderedData'\n",
        "saving_folder = '/feddl/work/miriam/orderedData'\n",
        "test_folder = '/feddl/work/miriam/test'\n",
        "\n",
        "# RETRIEVING ALL PATHS IN THE READING FOLDER - DATAFRAMES\n",
        "formatting_paths = get_filePaths( f'hdfs dfs -ls {reading_folder}/*/*') # get all the paths to day-folders\n",
        "formatting_months_paths = get_filePaths( f'hdfs dfs -ls {reading_folder}/*/') # get all the paths to month folders\n",
        "\n",
        "# TOTAL NUMBER OF FILE PATHS TO BE READ\n",
        "#num_paths = len(formatting_months_paths)\n",
        "num_paths = len(formatting_paths)\n",
        "print(num_paths)\n",
        "\n",
        "for p in range(1, num_paths):    \n",
        "    \n",
        "    # READ FILE  # formatting_months_paths[p] if enough memory\n",
        "    path = formatting_paths[p] \n",
        "    \n",
        "    print(path)\n",
        "    df = spark.read.parquet(path)\n",
        "    \n",
        "    # DROP UNUSEFUL COLUMNS\n",
        "    df = df.select([ 'Time-Quarter', 'Dato', 'InstallationAdresse', 'PostNr', 'MeterValue', 'MeasuredSpecies', 'MeterRegister', 'MeasuredProperty', 'ProductcomponentType'])\n",
        "\n",
        "    # MERGE ADDRESS AND POST CODE\n",
        "    df = merge_cols(df, 'InstallationAdresse', 'PostNr', 'InstallationAdresse', sep=' ')\n",
        "    \n",
        "    # CREATE HOUR COLUMN \n",
        "    df = col_to_hour(df, \"Time-Quarter\", \"Hour\")\n",
        "    \n",
        "    # FILTER OUT ONLY ROWS WHERE ADDRESS IS NOT EMPTY AND NOT ?\n",
        "    #df = df.where( col('InstallationAdresse').isNotNull() & (col('InstallationAdresse') != '?'))\n",
        "    \n",
        "    # ADD COLUMN MEASUREMENT TO FILTER SPECIFIC VALUES MEASURED\n",
        "    # check column names?\n",
        "    df = df.withColumn('Measurement', when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty')=='15minTimeseries-A-') & (col('MeterRegister')=='Active energy A- 15'), 'EL_A_kWh')\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='15minTimeseries-E-01') & (col('MeterRegister')=='Active energy A+ 15', 'EL_E_kWh' )\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 1 - value 4 60', 'EL_Fv_m3' )\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'EL_Va_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 1 - value 4 60', 'H_M1v4_C' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 3 - value 4 60', 'H_M3v4_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 2 60', 'W_M2v2_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'W_M2v1_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'W_cM2v1_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 2 60', 'W_cM2v2_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Power') & (col('MeasuredProperty') =='15minTimeseries-A-') & (col('MeterRegister') =='Active energy A- 15', 'P_A_kWh' )\n",
        "        .when(col('MeasuredSpecies')=='Power') & (col('MeasuredProperty') =='15minTimeseries-E-01') & (col('MeterRegister') =='Active energy A+ 15', 'P_E_KWh' ))\n",
        "    \n",
        "    # FILTER OUT ROWS WITH NULL VALUES IN THE NEW COLUMN Measurement\n",
        "    # in other words keep only the specified measurement selection above\n",
        "    df = df.where(df.Measurement.isNotNull())\n",
        "\n",
        "    # DEBUGGING, IF NEEDED\n",
        "    #print(df.show(2, truncate=False))\n",
        "    \n",
        "    # COLUMNS MERGING/DROPPING, FORWARD FILL INTEGER VALUES, HOURLY AGGREGATION\n",
        "    df = clean_agg(df, ff=True)\n",
        "    \n",
        "    # BUILD PATH TO SAVE CLEANED DATA\n",
        "    chrono_folder = path.split('g')[-1]\n",
        "    \n",
        "    # CREATE SAVING PATH\n",
        "    saving_path = saving_folder + chrono_folder\n",
        "    \n",
        "    # SAVE DAY AS PARQUET FILE\n",
        "    df.write.parquet(saving_path) # ADD , mode='overwrite' IF NEEDED\n",
        "    \n",
        "    # DEBUGGING\n",
        "    print([p,'SAVED', saving_path])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}