{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataCleaning",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miriammazzeo95/BigData_and_Timeseries_in_Pyspark/blob/main/DataCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Set Up Pyspark, Imports and Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5RuRg7H9Dwk",
        "outputId": "a0543c15-c6c4-40e2-a284-0adb52e8165b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u59MJyvdquRr",
        "outputId": "1320fdfd-dcc2-41a8-9f9b-7b46d5ea35bf"
      },
      "source": [
        "# TO CHANGE CURRENT DIR\n",
        "# %cd /content/drive/MyDrive/Colab\\ Notebooks/Big\\ Data\\ with\\ Pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Big Data with Pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIV8oOhQsnUA"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# https://colab.research.google.com/drive/1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA?usp=sharing\n",
        "colb_script_id = '1YGZCoCGw632dFe_yxAViQYIsXnFKqEdA'\n",
        "link_to_file_in_drive = drive.CreateFile({'id':colb_script_id})\n",
        "\n",
        "link_to_file_in_drive.GetContentFile('Pyspark_ConfigurationImportsFunctions.ipynb') # creates a local copy in the VM\n",
        "!jupyter nbconvert --to python 'Pyspark_ConfigurationImportsFunctions.ipynb' # converts the local copy from notebook to .py\n",
        "!rm Pyspark_ConfigurationImportsFunctions.ipynb # deletes the local copy\n",
        "import Pyspark_ConfigurationImportsFunctions as pyspark_config # imports everything from the script\n",
        "dir(pyspark_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7PoAow1Gro0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXiHiN4vKHXf"
      },
      "source": [
        "# **Cleaning Time Series energy data and saving them in Parqet Dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cEGYWlyO4iX"
      },
      "source": [
        "def clean_agg(df, ff=True):\n",
        "    \n",
        "    # UNCOMMENT WHENEVER THERE ARE EMPTY PARQUET FILE\n",
        "    #if df.first() == None:\n",
        "    #    remove_file(p)\n",
        "    #    print(f'REMOVED {p} ')\n",
        "    #    return df\n",
        "    #else:\n",
        "    # ...\n",
        "    \n",
        "    # DROP COLS\n",
        "    df=df.drop('MeasuredSpecies', 'MeasuredProperty', 'MeterRegister') \n",
        "    if ff:\n",
        "        # FORWARD FILL (NUMERICAL VALUES)\n",
        "        df = forward_fill_int(df, 'MeterValue',  ['Measurement','Time-Quarter'], 'MeterValue')\n",
        "    # AGGREGATION OF MeterValue ON HOUR (keeping same DATETIME, MEASUREMENT AND COMPONENT TYPES, ADDRESS)\n",
        "    df = df.withColumnRenamed(\"ProductcomponentType\",\"Component\")\n",
        "    df = df.groupBy('Dato', 'Hour',  'InstallationAdresse', 'Measurement', 'Component').sum()\n",
        "    df = df.withColumnRenamed(\"sum(MeterValue)\",\"Value\")\n",
        "    # DROP COLUMN\n",
        "    df = df.drop(col(\"Time-Quarter\"))\n",
        "    # ADD DAY COLUMN IF NECESSARY\n",
        "    #df = df.withColumn('day', dayofmonth('Dato'))\n",
        "    return df\n",
        "\n",
        "## SET UP READING PATH FROM HDFS - WHERE DATAFRAMES ARE STORED IN PARQUET\n",
        "# dataframes are stored in year=../month=../day=.. folders\n",
        "reading_folder = '/feddl/work/miriam/orderedData'\n",
        "saving_folder = '/feddl/work/miriam/orderedData'\n",
        "test_folder = '/feddl/work/miriam/test'\n",
        "\n",
        "# RETRIEVING ALL PATHS IN THE READING FOLDER - DATAFRAMES\n",
        "formatting_paths = get_filePaths( f'hdfs dfs -ls {reading_folder}/*/*') # get all the paths to day-folders\n",
        "formatting_months_paths = get_filePaths( f'hdfs dfs -ls {reading_folder}/*/') # get all the paths to month folders\n",
        "\n",
        "# TOTAL NUMBER OF FILE PATHS TO BE READ\n",
        "#num_paths = len(formatting_months_paths)\n",
        "num_paths = len(formatting_paths)\n",
        "print(num_paths)\n",
        "\n",
        "for p in range(1, num_paths):    \n",
        "    \n",
        "    # READ FILE  # formatting_months_paths[p] if enough memory\n",
        "    path = formatting_paths[p] \n",
        "    \n",
        "    print(path)\n",
        "    df = spark.read.parquet(path)\n",
        "    \n",
        "    # DROP UNUSEFUL COLUMNS\n",
        "    df = df.select([ 'Time-Quarter', 'Dato', 'InstallationAdresse', 'PostNr', 'MeterValue', 'MeasuredSpecies', 'MeterRegister', 'MeasuredProperty', 'ProductcomponentType'])\n",
        "\n",
        "    # MERGE ADDRESS AND POST CODE\n",
        "    df = merge_cols(df, 'InstallationAdresse', 'PostNr', 'InstallationAdresse', sep=' ')\n",
        "    \n",
        "    # CREATE HOUR COLUMN \n",
        "    df = col_to_hour(df, \"Time-Quarter\", \"Hour\")\n",
        "    \n",
        "    # FILTER OUT ONLY ROWS WHERE ADDRESS IS NOT EMPTY AND NOT ?\n",
        "    #df = df.where( col('InstallationAdresse').isNotNull() & (col('InstallationAdresse') != '?'))\n",
        "    \n",
        "    # ADD COLUMN MEASUREMENT TO FILTER SPECIFIC VALUES MEASURED\n",
        "    # check column names?\n",
        "    df = df.withColumn('Measurement', when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty')=='15minTimeseries-A-') & (col('MeterRegister')=='Active energy A- 15'), 'EL_A_kWh')\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='15minTimeseries-E-01') & (col('MeterRegister')=='Active energy A+ 15', 'EL_E_kWh' )\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 1 - value 4 60', 'EL_Fv_m3' )\n",
        "        .when(col('MeasuredSpecies')=='EL Meter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'EL_Va_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 1 - value 4 60', 'H_M1v4_C' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Fv-01') & (col('MeterRegister') =='Multienergyunit 3 - value 4 60', 'H_M3v4_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 2 60', 'W_M2v2_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'W_M2v1_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 1 60', 'W_cM2v1_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Watermeter') & (col('MeasuredProperty') =='Timeseries-Va-01') & (col('MeterRegister') =='Multienergyunit 2 - value 2 60', 'W_cM2v2_m3' )\n",
        "        .when(col('MeasuredSpecies')=='Power') & (col('MeasuredProperty') =='15minTimeseries-A-') & (col('MeterRegister') =='Active energy A- 15', 'P_A_kWh' )\n",
        "        .when(col('MeasuredSpecies')=='Power') & (col('MeasuredProperty') =='15minTimeseries-E-01') & (col('MeterRegister') =='Active energy A+ 15', 'P_E_KWh' ))\n",
        "    \n",
        "    # FILTER OUT ROWS WITH NULL VALUES IN THE NEW COLUMN Measurement\n",
        "    # in other words keep only the specified measurement selection above\n",
        "    df = df.where(df.Measurement.isNotNull())\n",
        "\n",
        "    # DEBUGGING, IF NEEDED\n",
        "    #print(df.show(2, truncate=False))\n",
        "    \n",
        "    # COLUMNS MERGING/DROPPING, FORWARD FILL INTEGER VALUES, HOURLY AGGREGATION\n",
        "    df = clean_agg(df, ff=True)\n",
        "    \n",
        "    # BUILD PATH TO SAVE CLEANED DATA\n",
        "    chrono_folder = path.split('g')[-1]\n",
        "    \n",
        "    # CREATE SAVING PATH\n",
        "    saving_path = saving_folder + chrono_folder\n",
        "    \n",
        "    # SAVE DAY AS PARQUET FILE\n",
        "    df.write.parquet(saving_path) # ADD , mode='overwrite' IF NEEDED\n",
        "    \n",
        "    # DEBUGGING\n",
        "    print([p,'SAVED', saving_path])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}